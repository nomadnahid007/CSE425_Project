# -*- coding: utf-8 -*-
"""CSE425 Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/nohid007/cse425-project.f2e2c6ce-e35e-4f0f-9e89-2f062c6b2dc8.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250518/auto/storage/goog4_request%26X-Goog-Date%3D20250518T212428Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D9ab7aa95cfde537a2e7a0583a84c2569b95d7cfea47e4d1d9ff4e75ef453b846bc378624be5a7dd515a49d15a47c214b6a1ee51a9e0f1f7f55769b08ef13847bbbc7c7a7b0145774d4375bbd0fb977ab646b6e5e59c2ccdb0f9be0930b6b326e6c28d11cd077e18b7156d3a1c8a21196c2257a7ad9931ee8477030d1bb3ceecc91788d22baa88c703923c013fcaa8258e77ba6f79cd8d6640aa3b672d558ed5bf232c251268ff6b8b2eab9172017448ea036d5836f1b7b72d2646898ae9f307f294daa3d32723e122904ea5ee53634635b9bb1107b941446959b1485dce46154ee2616e71dfd9c51b5df92752979122cfefcb6fbfdb4cdf741592a5ef1972b45
"""

pip install -q datasets sentence-transformers scikit-learn pandas numpy

"""# **Libraries**"""

from datasets import load_dataset
import pandas as pd
import random
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
import numpy as np
import torch
import torch.nn as nn
from sklearn.cluster import KMeans, SpectralClustering, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, silhouette_score, davies_bouldin_score, confusion_matrix
from scipy.optimize import linear_sum_assignment
from torch.utils.data import TensorDataset, DataLoader

"""# Dataset Loading: DBPedia"""

# Load DBpedia
dataset = load_dataset("dbpedia_14", split="train")
texts = [x['content'] for x in dataset]

 # For evaluation only
labels = [x['label'] for x in dataset]

print(f"Total samples: {len(texts)}")
print("Sample text:\n", texts[0])

"""# Subsampling the Dataset"""

random.seed(42)
sample_size = 10000
combined = list(zip(texts, labels))
random.shuffle(combined)
texts, labels = zip(*combined[:sample_size])

"""# Creating the Embeddings"""

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(texts, show_progress_bar=True, batch_size=64)

"""# Normalize and Save Embeddings"""

scaler = StandardScaler()
embeddings_scaled = scaler.fit_transform(embeddings)

# Save
np.save("dbpedia_embeddings.npy", embeddings_scaled)
np.save("dbpedia_labels.npy", labels)

print("Embeddings saved. Shape:", embeddings_scaled.shape)

"""**Dataset Analysis (for Report)**"""

import collections
sentence_lengths = [len(t.split()) for t in texts]

import warnings
warnings.filterwarnings("ignore", category=FutureWarning)


plt.figure(figsize=(10,5))
sns.histplot(sentence_lengths, bins=50, kde=True)
plt.title("Sentence Length Distribution")
plt.xlabel("Words per Sentence")
plt.ylabel("Frequency")
plt.show()

label_counts = collections.Counter(labels)
print("Class Distribution:", label_counts)

"""# Neural Network: CLANet
- This is a custom encoder architecture designed to project sentence embeddings into a structured latent space. It includes BatchNorm, Dropout, and a contrastive projection head.
"""

import torch.nn.functional as F

class CLANet(nn.Module):
    def __init__(self, input_dim=384, latent_dim=64, hidden_dim=512, dropout=0.3):
        super(CLANet, self).__init__()

        # Encoder
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, latent_dim)
        )

        # Projection Head (for contrastive loss)
        self.projection = nn.Sequential(
            nn.Linear(latent_dim, latent_dim),
            nn.ReLU(),
            nn.Linear(latent_dim, latent_dim)
        )

    def forward(self, x):
        z = self.encoder(x)
        p = self.projection(z)
        return z, p

"""# CLANet with a Supervised Version"""

class SupervisedCLANet(nn.Module):
    def __init__(self, input_dim=384, latent_dim=64, hidden_dim=512, num_classes=14):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, latent_dim)
        )
        self.classifier = nn.Linear(latent_dim, num_classes)

    def forward(self, x):
        z = self.encoder(x)
        logits = self.classifier(z)
        return z, logits

"""# Counting Parameters of the Model"""

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

model = SupervisedCLANet()
print(f"Total trainable parameters: {count_parameters(model):,}")

"""# Training Hyperparameters"""

LATENT_DIM = 64
HIDDEN_DIM = 512
DROPOUT = 0.3
LR = 1e-3
TEMPERATURE = 0.5

"""# Contrastive Loss"""

import torch.nn.functional as F
import torch

def contrastive_loss(z_i, z_j, temperature=0.5):
    batch_size = z_i.size(0)
    z_i = F.normalize(z_i, dim=1)
    z_j = F.normalize(z_j, dim=1)

    representations = torch.cat([z_i, z_j], dim=0)  # [2N, D]
    similarity_matrix = torch.matmul(representations, representations.T)  # [2N, 2N]

    mask = torch.eye(2 * batch_size, device=similarity_matrix.device).bool()
    similarity_matrix = similarity_matrix[~mask].view(2 * batch_size, -1)

    positives = torch.sum(z_i * z_j, dim=1).repeat(2)
    numerator = torch.exp(positives / temperature)

    denominator = torch.sum(torch.exp(similarity_matrix / temperature), dim=1)
    loss = -torch.log(numerator / denominator)
    return loss.mean()

"""**Training Data Preparation Load**"""

embeddings_scaled = np.load("dbpedia_embeddings.npy")

"""# Prepare Training Dataset with Labels"""

X = torch.tensor(embeddings_scaled, dtype=torch.float32)
y = torch.tensor(labels, dtype=torch.long)
dataset = TensorDataset(X, y)
dataloader = DataLoader(dataset, batch_size=128, shuffle=True)

"""# Model Training Begins"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = SupervisedCLANet(input_dim=384, latent_dim=64, hidden_dim=512, num_classes=14).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
criterion = nn.CrossEntropyLoss()

epochs = 40
losses = []

for epoch in range(epochs):
    model.train()
    total_loss = 0

    for x_batch, y_batch in dataloader:
        x_batch = x_batch.to(device)
        y_batch = y_batch.to(device)

        z, logits = model(x_batch)
        loss = criterion(logits, y_batch)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / len(dataloader)
    losses.append(avg_loss)
    print(f"Epoch {epoch+1}/{epochs}, Supervised Loss: {avg_loss:.4f}")

"""Loss Curve"""

plt.figure(figsize=(8, 4))
plt.plot(range(1, epochs + 1), losses, marker='o')
plt.title("Contrastive Loss Curve (Training)")
plt.xlabel("Epoch")
plt.ylabel("Average Loss")
plt.grid(True)
plt.show()

"""#  Saving the Latent Representations for Clustering"""

model.eval()
with torch.no_grad():
    latent_all = []

    for x_batch, _ in dataloader:
        x_batch = x_batch.to(device)
        z, _ = model(x_batch)
        latent_all.append(z.cpu())

    z = torch.cat(latent_all, dim=0).numpy()
    np.save("latent_z.npy", z)

print("Saved latent space representations.")

"""# Loading the Latent Representations"""

z = np.load("latent_z.npy")

"""# Note: Loading the labels only for evaluation"""

y_true = np.load("dbpedia_labels.npy")

"""# Clustering Algorithms

**K-Means**
"""

def cluster_kmeans(z, n_clusters):
    model = KMeans(n_clusters=n_clusters, random_state=42)
    return model.fit_predict(z)

"""**Spectral Clustering**"""

def cluster_spectral(z, n_clusters):
    model = SpectralClustering(n_clusters=n_clusters, affinity='nearest_neighbors', random_state=42)
    return model.fit_predict(z)

"""**Agglomerative Clustering**"""

def cluster_agglomerative(z, n_clusters):
    model = AgglomerativeClustering(n_clusters=n_clusters)
    return model.fit_predict(z)

"""**Gaussian Mixture Model**"""

def cluster_gmm(z, n_clusters):
    model = GaussianMixture(n_components=n_clusters, random_state=42)
    return model.fit(z).predict(z)

"""# Custom Model: Adaptive DEC Clustering"""

def entropy(p):

    return -torch.sum(p * torch.log(p + 1e-8), dim=1)

def student_t_dist(distances):
    q = (1.0 + distances**2)**-1
    return q / q.sum(dim=1, keepdim=True)

def cluster_adec_pp(z, n_clusters=14, epochs=10, confidence_threshold=0.15, initial_temp=1.0, final_temp=0.1, prune_variance_thresh=5.0):
    z_tensor = torch.tensor(z, dtype=torch.float32).to('cuda' if torch.cuda.is_available() else 'cpu')
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    y_pred = kmeans.fit_predict(z)
    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(z_tensor.device)

    temp_schedule = torch.linspace(initial_temp, final_temp, steps=epochs).to(z_tensor.device)

    for epoch in range(epochs):
        temp = temp_schedule[epoch]

        # Soft assignment with annealing temperature
        dist = torch.cdist(z_tensor, centroids)
        q = student_t_dist(dist / temp)

        # Entropy-based confidence scoring
        conf = 1 - entropy(q)
        confidence, predicted = torch.max(q, dim=1)
        mask = confidence > confidence_threshold

        # Entropy-weighted cluster updates
        for i in range(n_clusters):
            cluster_points = z_tensor[mask & (predicted == i)]
            cluster_weights = conf[mask & (predicted == i)].unsqueeze(1)

            if cluster_points.shape[0] > 0:
                weighted_mean = torch.sum(cluster_points * cluster_weights, dim=0) / torch.sum(cluster_weights)
                centroids[i] = weighted_mean

        # Optional: Prune unstable clusters
        for i in range(n_clusters):
            points = z_tensor[predicted == i]
            if points.shape[0] > 0:
                variance = torch.var(points, dim=0).mean()
                if variance > prune_variance_thresh:
                    centroids[i] = torch.randn_like(centroids[i])  # re-init centroid
                    print(f"Pruned cluster {i} at epoch {epoch+1} (variance: {variance:.4f})")

        if epoch % 2 == 0:
            print(f"[A-DEC++] Epoch {epoch+1}: {mask.sum().item()} confident samples retained")
        if epoch == 3:
            print("Reached max confident sample count. Freezing pruning to avoid collapse.")
            break

    return predicted.cpu().numpy()

"""# Adaptive DEC Clustering Model (FIXED)
* The previous one model was not reaching the benchmark, so I decided to refine that model slightly in a new cell
"""

def entropy(p):
    return -torch.sum(p * torch.log(p + 1e-8), dim=1)

def student_t_dist(distances):
    q = (1.0 + distances**2)**-1
    return q / q.sum(dim=1, keepdim=True)

def cluster_adec_pp_strong(
    z,
    n_clusters=14,
    epochs=10,
    confidence_threshold=0.15,
    initial_temp=1.0,
    final_temp=0.1,
    prune_variance_thresh=5.0
):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    z_tensor = torch.tensor(z, dtype=torch.float32).to(device)

    # Init KMeans
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    y_pred = kmeans.fit_predict(z)
    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)

    # Log-scale temperature schedule
    temp_schedule = torch.exp(torch.linspace(np.log(initial_temp), np.log(final_temp), epochs)).to(device)

    # Dynamic early stopping
    stable_epochs = 0
    max_stable_epochs = 2
    stable_conf_threshold = 0.40
    stable_retention_threshold = 0.95

    for epoch in range(epochs):
        temp = temp_schedule[epoch]
        dist = torch.cdist(z_tensor, centroids)
        q = student_t_dist(dist / temp)

        conf = 1 - entropy(q)
        confidence, predicted = torch.max(q, dim=1)
        mask = confidence > confidence_threshold

        avg_conf = confidence.mean().item()
        retained_frac = mask.sum().item() / len(z_tensor)

        print(f"[Epoch {epoch+1}] Avg Confidence: {avg_conf:.4f} | Retained: {mask.sum().item()}")

        # Update centroids
        for i in range(n_clusters):
            cluster_points = z_tensor[mask & (predicted == i)]
            cluster_weights = conf[mask & (predicted == i)].unsqueeze(1)

            if cluster_points.shape[0] > 0:
                weighted_mean = torch.sum(cluster_points * cluster_weights, dim=0) / torch.sum(cluster_weights)
                centroids[i] = weighted_mean

        # Adaptive pruning
        if epoch >= 2:
            for i in range(n_clusters):
                points = z_tensor[predicted == i]
                if points.shape[0] > 0:
                    variance = torch.var(points, dim=0).mean()
                    if variance > prune_variance_thresh:
                        centroids[i] = torch.randn_like(centroids[i])
                        print(f"  Pruned cluster {i} (variance: {variance:.4f})")

        # Dynamic early stopping logic
        if avg_conf >= stable_conf_threshold and retained_frac >= stable_retention_threshold:
            stable_epochs += 1
            if stable_epochs >= max_stable_epochs:
                print(f"Early stopping: clustering stabilized at Epoch {epoch+1}.")
                break
        else:
            stable_epochs = 0  # Reset if unstable

    return predicted.cpu().numpy()

"""# Evaluation Function"""

def evaluate_clusters(y_true, y_pred, name="Model"):
    # Hungarian Accuracy
    D = confusion_matrix(y_true, y_pred)
    row_ind, col_ind = linear_sum_assignment(-D)
    hungarian_acc = D[row_ind, col_ind].sum() / D.sum()

    # Other metrics
    nmi = normalized_mutual_info_score(y_true, y_pred)
    ari = adjusted_rand_score(y_true, y_pred)
    silhouette = silhouette_score(z, y_pred)
    dbi = davies_bouldin_score(z, y_pred)

    print(f"\n{name} Evaluation")
    print(f"Accuracy (Hungarian): {hungarian_acc:.4f}")
    print(f"NMI: {nmi:.4f} | ARI: {ari:.4f}")
    print(f"Silhouette Score: {silhouette:.4f} | Davies-Bouldin Index: {dbi:.4f}")
    return hungarian_acc, nmi, ari, silhouette, dbi

"""# Clustering Algorithms: Evaluation Part"""

num_clusters = len(set(y_true))

evaluate_clusters(y_true, cluster_kmeans(z, num_clusters), "KMeans")
evaluate_clusters(y_true, cluster_spectral(z, num_clusters), "Spectral Clustering")
evaluate_clusters(y_true, cluster_agglomerative(z, num_clusters), "Agglomerative Clustering")
evaluate_clusters(y_true, cluster_gmm(z, num_clusters), "Gaussian Mixture Model")
evaluate_clusters(y_true, cluster_adec_pp(z, n_clusters=len(set(y_true))), "A-DEC++")
evaluate_clusters(y_true, cluster_adec_pp_strong(z, n_clusters=len(set(y_true))), "A-DEC++ (Fixed)")

from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

def plot_tsne(z, labels, title="t-SNE of Latent Space", save=False):
    tsne = TSNE(n_components=2, perplexity=30, random_state=42)
    z_2d = tsne.fit_transform(z)

    plt.figure(figsize=(8, 6))
    scatter = plt.scatter(z_2d[:, 0], z_2d[:, 1], c=labels, cmap='tab20', s=8)
    plt.title(title)
    plt.xlabel("t-SNE Dim 1")
    plt.ylabel("t-SNE Dim 2")
    plt.colorbar(scatter)
    if save:
        plt.savefig(f"{title.replace(' ', '_')}.png", dpi=300)
    plt.grid(True)
    plt.show()

# Visualize your clusters (unsupervised)
adec_labels = cluster_adec_pp_strong(z, n_clusters=14)

plot_tsne(z, adec_labels, title="A-DEC++ Cluster Visualization")

plot_tsne(z, y_true, title="Ground Truth Categories")

"""# Ablation Study

"""

def cluster_adec_ablation(
    z,
    n_clusters=14,
    epochs=10,
    confidence_threshold=0.15,
    initial_temp=1.0,
    final_temp=0.1,
    prune=True,
    entropy_weight=True,
    temp_schedule=True
):
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    z_tensor = torch.tensor(z, dtype=torch.float32).to(device)

    # Initialize KMeans centroids
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    y_pred = kmeans.fit_predict(z)
    centroids = torch.tensor(kmeans.cluster_centers_, dtype=torch.float32).to(device)

    # Temperature schedule
    if temp_schedule:
        temp_schedule = torch.exp(torch.linspace(np.log(initial_temp), np.log(final_temp), epochs)).to(device)
    else:
        temp_schedule = torch.ones(epochs).to(device)

    # Dynamic early stopping
    stable_epochs = 0
    max_stable_epochs = 2
    stable_conf_threshold = 0.40
    stable_retention_threshold = 0.95

    for epoch in range(epochs):
        temp = temp_schedule[epoch]
        dist = torch.cdist(z_tensor, centroids)
        q = student_t_dist(dist / temp)

        # Entropy-weighted confidence
        conf = 1 - entropy(q) if entropy_weight else torch.ones(q.shape[0]).to(device)
        confidence, predicted = torch.max(q, dim=1)
        mask = confidence > confidence_threshold

        avg_conf = confidence.mean().item()
        retained_frac = mask.sum().item() / len(z_tensor)

        print(f"[Epoch {epoch+1}] Avg Confidence: {avg_conf:.4f} | Retained: {mask.sum().item()}")

        # Update centroids using weighted mean
        for i in range(n_clusters):
            cluster_points = z_tensor[mask & (predicted == i)]
            cluster_weights = conf[mask & (predicted == i)].unsqueeze(1)

            if cluster_points.shape[0] > 0:
                weighted_mean = torch.sum(cluster_points * cluster_weights, dim=0) / torch.sum(cluster_weights)
                centroids[i] = weighted_mean

        # Optional pruning
        if prune and epoch >= 2:
            for i in range(n_clusters):
                points = z_tensor[predicted == i]
                if points.shape[0] > 0:
                    variance = torch.var(points, dim=0).mean()
                    if variance > 5.0:
                        centroids[i] = torch.randn_like(centroids[i])
                        print(f"  Pruned cluster {i} (variance: {variance:.4f})")

        # Dynamic early stopping logic
        if avg_conf >= stable_conf_threshold and retained_frac >= stable_retention_threshold:
            stable_epochs += 1
            if stable_epochs >= max_stable_epochs:
                print(f"Early stopping: clustering stabilized at Epoch {epoch+1}.")
                break
        else:
            stable_epochs = 0  # reset counter if unstable

    return predicted.cpu().numpy()

for cfg in configs:
    name = cfg["name"]
    params = {k: v for k, v in cfg.items() if k != "name"}

    print(f"\n--- Running {name} ---")
    preds = cluster_adec_ablation(z, n_clusters=14, **params)
    score = evaluate_clusters(y_true, preds, name)
    results.append((name,) + score)

"""# To show my method generalizes, applying the model on a new dataset."""

from sklearn.datasets import fetch_20newsgroups
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import re
import nltk
nltk.download('stopwords')
from nltk.corpus import stopwords

"""**Preprocessing**"""

stop_words = set(stopwords.words('english'))

def clean_text(text):
    text = re.sub(r'\W+', ' ', text)             # Remove non-words
    text = text.lower()
    tokens = text.split()
    tokens = [w for w in tokens if w not in stop_words and len(w) > 2]
    return ' '.join(tokens)

"""**Loading the dataset**"""

from sklearn.decomposition import PCA

newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))
texts_20 = [clean_text(doc) for doc in newsgroups.data]
labels_20 = newsgroups.target
labels_20_trimmed = labels_20[:10000]


sbert = SentenceTransformer('all-MiniLM-L6-v2')
X_sbert = sbert.encode(texts_20, batch_size=64, show_progress_bar=True)


pca = PCA(n_components=256, random_state=42)
z_20 = pca.fit_transform(X_sbert)

"""# Run A-DEC++ on new dataset"""

y_true_20 = labels
z_20 = X_reduced

adec_20_labels = cluster_adec_pp_strong(z_20, n_clusters=20)
evaluate_clusters(labels_20_trimmed, adec_20_labels, name="A-DEC++ (20 Newsgroups)")

"""**Run KMeans on the new dataset**"""

y_true_20 = labels_20[:10000]
z_20 = z_20[:10000]
evaluate_clusters(y_true_20, KMeans(n_clusters=20, random_state=42).fit_predict(z_20), name="KMeans")

# Agglomerative Clustering
evaluate_clusters(y_true_20, AgglomerativeClustering(n_clusters=20).fit_predict(z_20), name="Agglomerative")

# Spectral Clustering
evaluate_clusters(y_true_20, SpectralClustering(n_clusters=20, affinity='nearest_neighbors', random_state=42).fit_predict(z_20), name="Spectral")

# Gaussian Mixture Model
evaluate_clusters(y_true_20, GaussianMixture(n_components=20, random_state=42).fit(z_20).predict(z_20), name="GMM")